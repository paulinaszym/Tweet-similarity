{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autorzy: Paulina Szymanek, Tomasz Kurcaba IiAD 3 rok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15984/1395143212.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "dataframe=pd.read_csv(\"trump_insult_tweets_2014_to_2021.csv\", index_col=0)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jako postać polityczna Donald J. Trump używał Twittera do wychwalania, nagabywania, zabawiania, lobbowania, ustalania własnej wersji wydarzeń – i, co być może najbardziej znaczące, do wzmacniania swojej pogardy. Ten dataset dokumentuje ataki werbalne, które Trump opublikował na Twitterze, od ogłoszenia swojej kandydatury w czerwcu 2015 r. do 8 stycznia, kiedy Twitter na stałe go zabronił.\n",
    "Źródło: Archiwum Trumpa na Twitterze\n",
    "Ten zbiór danych został pobrany z The New York Times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zbiór ten zawiera kolumny: date, target, insult, tweet   \n",
    "date - data tweeta   \n",
    "target - cel, osoba/tematyka w jaką wymierzony był tweet   \n",
    "insult - użyta \"zniewaga\"   \n",
    "tweet - cały tweet  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitt_value_counts =dataframe['target'].value_counts()\n",
    "twitt_value_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że Donald Trump najczęściej kierował tweety w stronę szeroko pojętych mediów, demokratów i hillary clinton. Wysoko jest jego kolejny rywal czyli Joe Biden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'@\\w+')\n",
    "\n",
    "dest_list = []\n",
    "temp = ''\n",
    "for val in dataframe['tweet']:\n",
    "    temp = regex.findall(str(val))\n",
    "    temp_2 = ''\n",
    "    if temp:\n",
    "        if len(temp) > 1:\n",
    "            temp_2 = ' '.join(temp)\n",
    "        else: \n",
    "            temp_2 = temp[0]\n",
    "    else:\n",
    "        temp_2 = 'None'\n",
    "    dest_list.append(temp_2)\n",
    "\n",
    "for i in range(10):\n",
    "    print(dest_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_array = np.array(dest_list)\n",
    "dataframe['mentions']= pd.Series(dest_array)\n",
    "dataframe['mentions'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = ['@nytimes', '@CNN', '@FoxNews', '@foxandfriends', '@JebBush'] \n",
    "counts = [236, 179, 172, 75, 43] # seats in assembly\n",
    "explode=(0.2, 0.2, 0.2, 0.2, 0.2) # distance from each other\n",
    "pie=plt.pie(counts,\n",
    "        explode=explode,\n",
    "        labels=best,\n",
    "        autopct='%.1f%%',\n",
    "        shadow=True,\n",
    "        startangle=45\n",
    ")\n",
    "plt.axis('equal')\n",
    "plt.title('Graph of top 5 mentions')\n",
    "plt.legend(pie[0], best, loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "text = dataframe.tweet.to_csv()\n",
    "wc  = WordCloud( background_color='white', stopwords=stopwords)\n",
    "wc.generate(text)\n",
    "plt.figure(figsize=(14,14))\n",
    "plt.imshow(wc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poglądowo spójrzymy na powyższy wykres który pokazuje występowanie słów/pojedynczych wyrazów w badanym zbiorze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęstsze wzmianki w tweetach prezydenta Trump to The New York Times, Jeb Bush (był gubernator Florydy), Fox & Friends (talkshow), Fox News Channel(stacja informacyjna), CNN(telewizja informacyjna)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celem niniejszego projektu będzie określenie podobieństwa między tweetem, który jest oczyszczony ze wszystkich znaków dodatkowych z wejściowych tweetem. Zostanie to zrobione przy użyciu macierzy funkcji TF-IDF.   \n",
    "TF to pozwala określić częstotliwość występowania danej frazy w konkretnym dokumencie. Jego zakres obejmuje wyłącznie jeden dokument, który ma być badany. Wartość ta jest wprost proporcjonalna do częstotliwości występowania wyrazu.Aby obliczyć TF, należy podzielić liczbę wystąpień frazy w dokumencie przez liczbę znajdujących się w nim wszystkich słów.   \n",
    "IDF natomiast  pozwala sprawdzić, jak często dany termin występuje we wszystkich dokumentach badanego korpusu językowego. W zależności od potrzeb może być chociażby TOP10 wyników wyszukiwania, zbiór artykułów naukowych z danej dziedziny, konkretny zbiór tekstów, itp. Jest to odwrotna częstotliwość wyrażenia, dlatego też im częściej dany wyraz pojawia się w korpusie, tym wynik IDF będzie niższy. We wzorze użyty jest logarytm.   \n",
    "Używaną przez nas metryką będzie podobieństwo cosinusowe. Metryka ta mierzy podobieństwo między dwoma wektorami wewnętrznej przestrzeni iloczynu. Jest mierzony przez cosinus kąta między dwoma wektorami i określa, czy dwa wektory wskazują mniej więcej w tym samym kierunku. Jest często używany do pomiaru podobieństwa dokumentów w analizie tekstu. Dokument może być reprezentowany przez tysiące atrybutów, z których każdy rejestruje częstotliwość występowania określonego słowa (takiego jak słowo kluczowe) lub frazy w dokumencie. Tak więc każdy dokument jest obiektem reprezentowanym przez tak zwany wektor częstotliwości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global variables\n",
    "STOP_WORDS=set(nltk.corpus.stopwords.words('english'))\n",
    "PORTER=nltk.stem.porter.PorterStemmer()\n",
    "LEMMATIZER=nltk.stem.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cleaning string\n",
    "def clean_string(string):\n",
    "    words_split=nltk.word_tokenize(string)\n",
    "    words_split=[re.sub(r\"[^a-zA-Z0-9]+\",'',word) for word in words_split]\n",
    "    words_split=list(filter(None,words_split))\n",
    "    words_split=[word.lower() for word in words_split]\n",
    "    words_clean=[word for word in words_split if word not in STOP_WORDS]\n",
    "    words_stemmed=[PORTER.stem(word) for word in words_clean]\n",
    "    output=' '.join(words_stemmed)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean_string(string):\n",
    "    words_split=nltk.word_tokenize(string)\n",
    "    words_split=[re.sub(r\"[^a-zA-Z0-9]+\",'',word) for word in words_split]\n",
    "    words_split=list(filter(None,words_split))\n",
    "    words_split=[word.lower() for word in words_split]\n",
    "    words_clean=[word for word in words_split if word not in STOP_WORDS]\n",
    "    words_stemmed=[LEMMATIZER.lemmatize(word) for word in words_clean]\n",
    "    output=' '.join(words_stemmed)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean columns\n",
    "dataframe['insult_cleaned']=dataframe['insult'].apply(lambda x: clean_string(x))\n",
    "dataframe['tweet_cleaned']=dataframe['tweet'].apply(lambda x: clean_string(x))\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframe['insult_cleaned'].unique())\n",
    "len(dataframe['tweet_cleaned'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insults = dataframe['insult_cleaned'].unique() #list of unique insults\n",
    "insults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframeNew = dataframe.groupby(['tweet_cleaned','target','tweet'])['insult_cleaned'].apply(list).reset_index(name='insult_cleaned')\n",
    "dataframeNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tfidf=vectorizer.fit_transform(dataframeNew['tweet_cleaned']).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating similarity measure\n",
    "cosine_similarity_measure=cosine_similarity(df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary with indexes of tweet that contain that insult\n",
    "key_indexes_dict={}\n",
    "for insult in insults:\n",
    "    key_indexes_dict[insult]=[]\n",
    "    for idx, row in enumerate(dataframeNew['insult_cleaned']):\n",
    "        if insult in row:\n",
    "                key_indexes_dict[insult].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_indexes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding dummy row for case when no tweet has the same set of insults\n",
    "dataframeNew.loc[dataframeNew.shape[0]-1,:]=['No match found','No match found','No match found','No match found']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guessed_text=[]\n",
    "sim_measure=[]\n",
    "target_bonus=0.1 #bonus for the same tweet target\n",
    "\n",
    "for text_idx, text_insult in enumerate(dataframeNew['insult_cleaned']):\n",
    "    if text_idx == dataframeNew.last_valid_index():\n",
    "        guessed_text.append(len(dataframeNew[\"tweet\"])-1)\n",
    "        sim_measure.append(0)\n",
    "        break;\n",
    "    rows_indexes=set(key_indexes_dict[text_insult[0]])\n",
    "    if len(text_insult)>1: #if more than one insult\n",
    "        for elem_in_key in range(1,len(text_insult)-1):\n",
    "            rows_indexes=rows_indexes&set(key_indexes_dict[text_insult[elem_in_key]])\n",
    "        rows_indexes=list(rows_indexes)\n",
    "        rows_indexes.sort()\n",
    "    if rows_indexes: #matches found\n",
    "        maximum_sim=0\n",
    "        maximum_idx=0\n",
    "        for row in rows_indexes:\n",
    "            if row == text_idx:\n",
    "                maximum_sim=0\n",
    "                maximum_idx=dataframeNew.last_valid_index()\n",
    "                continue;\n",
    "            cos_sim=cosine_similarity_measure[text_idx,row]\n",
    "            if dataframeNew.loc[row,'target']==dataframeNew.loc[text_idx,'target']:\n",
    "                cos_sim+=target_bonus\n",
    "            if cos_sim>maximum_sim:\n",
    "                maximum_sim=cos_sim\n",
    "                maximum_idx=row\n",
    "        guessed_text.append(maximum_idx)\n",
    "        sim_measure.append(maximum_sim)\n",
    "    else: \n",
    "        guessed_text.append(len(dataframeNew[\"tweet\"])-1)\n",
    "        sim_measure.append(0)\n",
    "\n",
    "dataframeNew['guessed_tweet']=dataframeNew.loc[guessed_text, 'tweet'].tolist()\n",
    "dataframeNew['similarity']=sim_measure\n",
    "dataframeNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer3 = CountVectorizer(ngram_range=(3,3), min_df=10)\n",
    "count_matrix3 = count_vectorizer3.fit_transform(dataframeNew['tweet'].apply(lambda x: basic_clean_string(x))).todense()\n",
    "\n",
    "counts3 = pd.DataFrame(count_matrix3, columns = count_vectorizer3.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts3.sum().sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najczęściej pojawiące się 3-gramy w badanym tekście. Możemy wywnioskować, że Donald Trump to bardzo wyrazista postać, która nie potrafi ugryźć się w język, nawet na twitterze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer2 = CountVectorizer(ngram_range=(2,2), min_df=10)\n",
    "count_matrix2 = count_vectorizer2.fit_transform(dataframeNew['tweet'].apply(lambda x: basic_clean_string(x))).todense()\n",
    "\n",
    "counts2 = pd.DataFrame(count_matrix2, columns = count_vectorizer2.get_feature_names())\n",
    "counts2.sum().sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podobne wnioski dla 2-gramów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer1 = CountVectorizer( min_df=50)\n",
    "count_matrix1 = count_vectorizer1.fit_transform(dataframeNew['tweet'].apply(lambda x: basic_clean_string(x))).todense()\n",
    "\n",
    "counts1 = pd.DataFrame(count_matrix1, columns = count_vectorizer1.get_feature_names())\n",
    "counts1.sum().sort_values(ascending=False)[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.bar(counts1.sum().sort_values(ascending=False)[0:15].index,counts1.sum().sort_values(ascending=False)[0:15])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.bar(counts2.sum().sort_values(ascending=False)[0:12].index,counts2.sum().sort_values(ascending=False)[0:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "plt.bar(counts3.sum().sort_values(ascending=False)[0:8].index,counts3.sum().sort_values(ascending=False)[0:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeNew['similarity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeNew['similarity'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że nasz model przyporządkował bardzo dużo tweetów jako niepodobnych, mimo to sporo jest takich mających wartości z przedziału 0.2-0.4 oraz w okolicach wartości 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
